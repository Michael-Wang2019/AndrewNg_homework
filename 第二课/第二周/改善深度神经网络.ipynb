{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在正式开始之前，我们说一下我们要做什么。我们需要做的是分割数据集和优化梯度下降算法，所以我们需要做以下几件事： \n",
    "#   1. 分割数据集 \n",
    "#   2. 优化梯度下降算法： \n",
    "#      2.1 不使用任何优化算法 \n",
    "#      2.2 mini-batch梯度下降法 \n",
    "#      2.3 使用具有动量的梯度下降算法 \n",
    "#      2.4 使用Adam算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "import opt_utils\n",
    "import testCase\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, leadrning_rate):\n",
    "#   \"\"\"\n",
    "#     使用梯度下降更新参数\n",
    "\n",
    "#     参数：\n",
    "#         parameters - 字典，包含了要更新的参数：\n",
    "#             parameters['W' + str(l)] = Wl\n",
    "#             parameters['b' + str(l)] = bl\n",
    "#         grads - 字典，包含了每一个梯度值用以更新参数\n",
    "#             grads['dW' + str(l)] = dWl\n",
    "#             grads['db' + str(l)] = dbl\n",
    "#         learning_rate - 学习率\n",
    "\n",
    "#     返回值：\n",
    "#         parameters - 字典，包含了更新后的参数\n",
    "#     \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    #更新每个参数\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+ 1 )] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]     \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试update_parameters_with_gd-------------\n",
      "W1 = [[ 1.63535156 -0.62320365 -0.53718766]\n",
      " [-1.07799357  0.85639907 -2.29470142]]\n",
      "b1 = [[ 1.74604067]\n",
      " [-0.75184921]]\n",
      "W2 = [[ 0.32171798 -0.25467393  1.46902454]\n",
      " [-2.05617317 -0.31554548 -0.3756023 ]\n",
      " [ 1.1404819  -1.09976462 -0.1612551 ]]\n",
      "b2 = [[-0.88020257]\n",
      " [ 0.02561572]\n",
      " [ 0.57539477]]\n"
     ]
    }
   ],
   "source": [
    "#测试update_parameters_with_gd\n",
    "print(\"-------------测试update_parameters_with_gd-------------\")\n",
    "parameters , grads , learning_rate = testCase.update_parameters_with_gd_test_case()\n",
    "parameters = update_parameters_with_gd(parameters,grads,learning_rate)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由梯度下降算法演变来的还有随机梯度下降（SGD）算法和小批量梯度下降算法，随机梯度下降（SGD），相当于小批量梯度下降，但是和mini-batch不同的是其中每个小批量(mini-batch)仅有1个样本，和梯度下降不同的是你一次只能在一个训练样本上计算梯度，而不是在整个训练集上计算梯度。我们来看一下它们的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #仅做比较，不运行。\n",
    "\n",
    "# #批量梯度下降，又叫梯度下降\n",
    "# X = data_input\n",
    "# Y = labels\n",
    "\n",
    "# parameters = initialize_parameters(layers_dims)\n",
    "# for i in range(0,num_iterations):\n",
    "#     #前向传播\n",
    "#     A,cache = forward_propagation(X,parameters)\n",
    "#     #计算损失\n",
    "#     cost = compute_cost(A,Y)\n",
    "#     #反向传播\n",
    "#     grads = backward_propagation(X,Y,cache)\n",
    "#     #更新参数\n",
    "#     parameters = update_parameters(parameters,grads)\n",
    "\n",
    "# #随机梯度下降算法：\n",
    "# X = data_input\n",
    "# Y = labels\n",
    "# parameters = initialize_parameters(layers_dims)\n",
    "# for i in (0,num_iterations):\n",
    "#     for j in m:\n",
    "#         #前向传播\n",
    "#         A,cache = forward_propagation(X,parameters)\n",
    "#         #计算成本\n",
    "#         cost = compute_cost(A,Y)\n",
    "#         #后向传播\n",
    "#         grads = backward_propagation(X,Y,cache)\n",
    "#         #更新参数\n",
    "#         parameters = update_parameters(parameters,grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "#   \"\"\"\n",
    "#     从（X，Y）中创建一个随机的mini-batch列表\n",
    "\n",
    "#     参数：\n",
    "#         X - 输入数据，维度为(输入节点数量，样本的数量)\n",
    "#         Y - 对应的是X的标签，【1 | 0】（蓝|红），维度为(1,样本的数量)\n",
    "#         mini_batch_size - 每个mini-batch的样本数量\n",
    "\n",
    "#     返回：\n",
    "#         mini-bacthes - 一个同步列表，维度为（mini_batch_X,mini_batch_Y）\n",
    "\n",
    "#   \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1] # x样本的数量\n",
    "    mini_batches = []\n",
    "    \n",
    "    # 第一步: 打乱顺序\n",
    "    permutation = list(np.random.permutation(m)) #  返回一个长度为m的随机数组\n",
    "    shuffled_X = X[:, permutation]  #将每一列数据按permutation的顺序重新排序\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    # 第二步：分割\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # 向下取整, 把训练集分成的份数\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    #如果训练集的大小刚好是mini_batch_size的整数倍，那么这里已经处理完了\n",
    "    #如果训练集的大小不是mini_batch_size的整数倍，那么最后肯定会剩下一些，我们要把它处理了\n",
    "    if m % mini_batch_size != 0:\n",
    "        #获取最后剩余的部分\n",
    "        mini_batch_X = shuffled_X[:, mini_batch_size * num_complete_minibatches :]\n",
    "        mini_batch_Y = shuffled_Y[:, mini_batch_size * num_complete_minibatches :]\n",
    "\n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试random_mini_batches-------------\n",
      "第1个mini_batch_X 的维度为： (12288, 64)\n",
      "第1个mini_batch_Y 的维度为： (1, 64)\n",
      "第2个mini_batch_X 的维度为： (12288, 64)\n",
      "第2个mini_batch_Y 的维度为： (1, 64)\n",
      "第3个mini_batch_X 的维度为： (12288, 20)\n",
      "第3个mini_batch_Y 的维度为： (1, 20)\n"
     ]
    }
   ],
   "source": [
    "#测试random_mini_batches\n",
    "print(\"-------------测试random_mini_batches-------------\")\n",
    "X_assess,Y_assess,mini_batch_size = testCase.random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess,Y_assess,mini_batch_size)\n",
    "\n",
    "print(\"第1个mini_batch_X 的维度为：\",mini_batches[0][0].shape)\n",
    "print(\"第1个mini_batch_Y 的维度为：\",mini_batches[0][1].shape)\n",
    "print(\"第2个mini_batch_X 的维度为：\",mini_batches[1][0].shape)\n",
    "print(\"第2个mini_batch_Y 的维度为：\",mini_batches[1][1].shape)\n",
    "print(\"第3个mini_batch_X 的维度为：\",mini_batches[2][0].shape)\n",
    "\n",
    "print(\"第3个mini_batch_Y 的维度为：\",mini_batches[2][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包含动量的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   由于小批量梯度下降只看到了一个子集的参数更新，更新的方向有一定的差异，所以小批量梯度下降的路径将“振荡地”走向收敛，使用动量可以减少这些振荡，动量考虑了过去的梯度以平滑更新， 我们将把以前梯度的方向存储在变量v中，从形式讲，这将是前面的梯度的指数加权平均值。我们也可以把V看作是滚下坡的速度，根据山坡的坡度建立动量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "#   \"\"\"\n",
    "#     初始化速度，velocity是一个字典：\n",
    "#         - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "#         - values:与相应的梯度/参数维度相同的值为零的矩阵。\n",
    "#     参数：\n",
    "#         parameters - 一个字典，包含了以下参数：\n",
    "#             parameters[\"W\" + str(l)] = Wl\n",
    "#             parameters[\"b\" + str(l)] = bl\n",
    "#     返回:\n",
    "#         v - 一个字典变量，包含了以下参数：\n",
    "#             v[\"dW\" + str(l)] = dWl的速度\n",
    "#             v[\"db\" + str(l)] = dbl的速度\n",
    "\n",
    "#   \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试initialize_velocity-------------\n",
      "v[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "v[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#测试initialize_velocity\n",
    "print(\"-------------测试initialize_velocity-------------\")\n",
    "parameters = testCase.initialize_velocity_test_case()\n",
    "v = initialize_velocity(parameters)\n",
    "\n",
    "print('v[\"dW1\"] = ' + str(v[\"dW1\"]))\n",
    "print('v[\"db1\"] = ' + str(v[\"db1\"]))\n",
    "print('v[\"dW2\"] = ' + str(v[\"dW2\"]))\n",
    "print('v[\"db2\"] = ' + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentun(parameters,grads,v,beta,learning_rate):\n",
    "#     \"\"\"\n",
    "#     使用动量更新参数\n",
    "#     参数：\n",
    "#         parameters - 一个字典类型的变量，包含了以下字段：\n",
    "#             parameters[\"W\" + str(l)] = Wl\n",
    "#             parameters[\"b\" + str(l)] = bl\n",
    "#         grads - 一个包含梯度值的字典变量，具有以下字段：\n",
    "#             grads[\"dW\" + str(l)] = dWl\n",
    "#             grads[\"db\" + str(l)] = dbl\n",
    "#         v - 包含当前速度的字典变量，具有以下字段：\n",
    "#             v[\"dW\" + str(l)] = ...\n",
    "#             v[\"db\" + str(l)] = ...\n",
    "#         beta - 超参数，动量，实数\n",
    "#         learning_rate - 学习率，实数\n",
    "#     返回：\n",
    "#         parameters - 更新后的参数字典\n",
    "#         v - 包含了更新后的速度变量\n",
    "#     \"\"\"\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        #计算速度\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "        #更新参数\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "\n",
    "    return parameters,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试update_parameters_with_momentun-------------\n",
      "W1 = [[ 1.62544598 -0.61290114 -0.52907334]\n",
      " [-1.07347112  0.86450677 -2.30085497]]\n",
      "b1 = [[ 1.74493465]\n",
      " [-0.76027113]]\n",
      "W2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n",
      " [-2.05974396 -0.32173003 -0.38320915]\n",
      " [ 1.13444069 -1.0998786  -0.1713109 ]]\n",
      "b2 = [[-0.87809283]\n",
      " [ 0.04055394]\n",
      " [ 0.58207317]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[0.02344157]\n",
      " [0.16598022]\n",
      " [0.07420442]]\n"
     ]
    }
   ],
   "source": [
    "#测试update_parameters_with_momentun\n",
    "print(\"-------------测试update_parameters_with_momentun-------------\")\n",
    "parameters,grads,v = testCase.update_parameters_with_momentum_test_case()\n",
    "update_parameters_with_momentun(parameters,grads,v,beta=0.9,learning_rate=0.01)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print('v[\"dW1\"] = ' + str(v[\"dW1\"]))\n",
    "print('v[\"db1\"] = ' + str(v[\"db1\"]))\n",
    "print('v[\"dW2\"] = ' + str(v[\"dW2\"]))\n",
    "print('v[\"db2\"] = ' + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam算法是训练神经网络中最有效的算法之一，它是RMSProp算法与Momentum算法的结合体。我们来看看它都干了些什么吧~ \n",
    "1. 计算以前的梯度的指数加权平均值，并将其存储在变量vv（偏差校正前）和vcorrectedvcorrected（偏差校正后）中。 \n",
    "2. 计算以前梯度的平方的指数加权平均值，并将其存储在变量ss（偏差校正前）和scorrectedscorrected（偏差校正后）中。 \n",
    "3. 根据1和2更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "#     \"\"\"\n",
    "#     初始化v和s，它们都是字典类型的变量，都包含了以下字段：\n",
    "#         - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "#         - values：与对应的梯度/参数相同维度的值为零的numpy矩阵\n",
    "\n",
    "#     参数：\n",
    "#         parameters - 包含了以下参数的字典变量：\n",
    "#             parameters[\"W\" + str(l)] = Wl\n",
    "#             parameters[\"b\" + str(l)] = bl\n",
    "#     返回：\n",
    "#         v - 包含梯度的指数加权平均值，字段如下：\n",
    "#             v[\"dW\" + str(l)] = ...\n",
    "#             v[\"db\" + str(l)] = ...\n",
    "#         s - 包含平方梯度的指数加权平均值，字段如下：\n",
    "#             s[\"dW\" + str(l)] = ...\n",
    "#             s[\"db\" + str(l)] = ...\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "    return (v,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试initialize_adam-------------\n",
      "v[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "v[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "s[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "s[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "s[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "s[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#测试initialize_adam\n",
    "print(\"-------------测试initialize_adam-------------\")\n",
    "parameters = testCase.initialize_adam_test_case()\n",
    "v,s = initialize_adam(parameters)\n",
    "\n",
    "print('v[\"dW1\"] = ' + str(v[\"dW1\"])) \n",
    "print('v[\"db1\"] = ' + str(v[\"db1\"])) \n",
    "print('v[\"dW2\"] = ' + str(v[\"dW2\"])) \n",
    "print('v[\"db2\"] = ' + str(v[\"db2\"])) \n",
    "print('s[\"dW1\"] = ' + str(s[\"dW1\"])) \n",
    "print('s[\"db1\"] = ' + str(s[\"db1\"])) \n",
    "print('s[\"dW2\"] = ' + str(s[\"dW2\"])) \n",
    "print('s[\"db2\"] = ' + str(s[\"db2\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters,grads,v,s,t,learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "#   \"\"\"\n",
    "#     使用Adam更新参数\n",
    "\n",
    "#     参数：\n",
    "#         parameters - 包含了以下字段的字典：\n",
    "#             parameters['W' + str(l)] = Wl\n",
    "#             parameters['b' + str(l)] = bl\n",
    "#         grads - 包含了梯度值的字典，有以下key值：\n",
    "#             grads['dW' + str(l)] = dWl\n",
    "#             grads['db' + str(l)] = dbl\n",
    "#         v - Adam的变量，第一个梯度的移动平均值，是一个字典类型的变量\n",
    "#         s - Adam的变量，平方梯度的移动平均值，是一个字典类型的变量\n",
    "#         t - 当前迭代的次数\n",
    "#         learning_rate - 学习率\n",
    "#         beta1 - 动量，超参数,用于第一阶段，使得曲线的Y值不从0开始（参见天气数据的那个图）\n",
    "#         beta2 - RMSprop的一个参数，超参数\n",
    "#         epsilon - 防止除零操作（分母为0）\n",
    "\n",
    "#     返回：\n",
    "#         parameters - 更新后的参数\n",
    "#         v - 第一个梯度的移动平均值，是一个字典类型的变量\n",
    "#         s - 平方梯度的移动平均值，是一个字典类型的变量\n",
    "#     \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {} #偏差修正后的值\n",
    "    s_corrected = {} #偏差修正后的值\n",
    "\n",
    "    for l in range(L):\n",
    "        #梯度的移动平均值,输入：\"v , grads , beta1\",输出：\" v \"\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "        #计算第一阶段的偏差修正后的估计值，输入\"v , beta1 , t\" , 输出：\"v_corrected\"\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1,t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1,t))\n",
    "\n",
    "        #计算平方梯度的移动平均值，输入：\"s, grads , beta2\"，输出：\"s\"\n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.square(grads[\"db\" + str(l + 1)])\n",
    "\n",
    "        #计算第二阶段的偏差修正后的估计值，输入：\"s , beta2 , t\"，输出：\"s_corrected\"\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2,t))\n",
    "\n",
    "        #更新参数，输入: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". 输出: \"parameters\".\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon))\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon))\n",
    "\n",
    "    return (parameters,v,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------测试update_with_parameters_with_adam-------------\n",
      "W1 = [[ 1.63178673 -0.61919778 -0.53561312]\n",
      " [-1.08040999  0.85796626 -2.29409733]]\n",
      "b1 = [[ 1.75225313]\n",
      " [-0.75376553]]\n",
      "W2 = [[ 0.32648046 -0.25681174  1.46954931]\n",
      " [-2.05269934 -0.31497584 -0.37661299]\n",
      " [ 1.14121081 -1.09245036 -0.16498684]]\n",
      "b2 = [[-0.88529978]\n",
      " [ 0.03477238]\n",
      " [ 0.57537385]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[0.02344157]\n",
      " [0.16598022]\n",
      " [0.07420442]]\n",
      "s[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n",
      " [0.0002525  0.00081154 0.00046748]]\n",
      "s[\"db1\"] = [[1.51020075e-05]\n",
      " [8.75664434e-04]]\n",
      "s[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n",
      " [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n",
      " [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\n",
      "s[\"db2\"] = [[5.49507194e-05]\n",
      " [2.75494327e-03]\n",
      " [5.50629536e-04]]\n"
     ]
    }
   ],
   "source": [
    "#测试update_with_parameters_with_adam\n",
    "print(\"-------------测试update_with_parameters_with_adam-------------\")\n",
    "parameters , grads , v , s = testCase.update_parameters_with_adam_test_case()\n",
    "update_parameters_with_adam(parameters,grads,v,s,t=2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print('v[\"dW1\"] = ' + str(v[\"dW1\"])) \n",
    "print('v[\"db1\"] = ' + str(v[\"db1\"])) \n",
    "print('v[\"dW2\"] = ' + str(v[\"dW2\"])) \n",
    "print('v[\"db2\"] = ' + str(v[\"db2\"])) \n",
    "print('s[\"dW1\"] = ' + str(s[\"dW1\"])) \n",
    "print('s[\"db1\"] = ' + str(s[\"db1\"])) \n",
    "print('s[\"dW2\"] = ' + str(s[\"dW2\"])) \n",
    "print('s[\"db2\"] = ' + str(s[\"db2\"])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用下面的“月亮（moon）”数据集来测试不同的优化方法。数据集被命名为“月亮”，因为这两个类的数据看起来有点像新月形的月亮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = opt_utils.load_dataset(is_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X,Y,layers_dims,optimizer,learning_rate=0.0007,\n",
    "          mini_batch_size=64,beta=0.9,beta1=0.9,beta2=0.999,\n",
    "          epsilon=1e-8,num_epochs=10000,print_cost=True,is_plot=True):\n",
    "\n",
    "#   \"\"\"\n",
    "#     可以运行在不同优化器模式下的3层神经网络模型。\n",
    "\n",
    "#     参数：\n",
    "#         X - 输入数据，维度为（2，输入的数据集里面样本数量）\n",
    "#         Y - 与X对应的标签\n",
    "#         layers_dims - 包含层数和节点数量的列表\n",
    "#         optimizer - 字符串类型的参数，用于选择优化类型，【 \"gd\" | \"momentum\" | \"adam\" 】\n",
    "#         learning_rate - 学习率\n",
    "#         mini_batch_size - 每个小批量数据集的大小\n",
    "#         beta - 用于动量优化的一个超参数\n",
    "#         beta1 - 用于计算梯度后的指数衰减的估计的超参数\n",
    "#         beta1 - 用于计算平方梯度后的指数衰减的估计的超参数\n",
    "#         epsilon - 用于在Adam中避免除零操作的超参数，一般不更改\n",
    "#         num_epochs - 整个训练集的遍历次数，（视频2.9学习率衰减，1分55秒处，视频中称作“代”）,相当于之前的num_iteration\n",
    "#         print_cost - 是否打印误差值，每遍历1000次数据集打印一次，但是每100次记录一个误差值，又称每1000代打印一次\n",
    "#         is_plot - 是否绘制出曲线图\n",
    "\n",
    "#     返回：\n",
    "#         parameters - 包含了学习后的参数\n",
    "\n",
    "#     \"\"\"\n",
    "    L = len(layers_dims)\n",
    "    costs = []\n",
    "    t = 0 #每学习完一个minibatch就增加1\n",
    "    seed = 10 #随机种子\n",
    "\n",
    "    #初始化参数\n",
    "    parameters = opt_utils.initialize_parameters(layers_dims)\n",
    "\n",
    "    #选择优化器\n",
    "    if optimizer == \"gd\":\n",
    "        pass #不使用任何优化器，直接使用梯度下降法\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters) #使用动量\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)#使用Adam优化\n",
    "    else:\n",
    "        print(\"optimizer参数错误，程序退出。\")\n",
    "        exit(1)\n",
    "\n",
    "    #开始学习\n",
    "    for i in range(num_epochs):\n",
    "        #定义随机 minibatches,我们在每次遍历数据集之后增加种子以重新排列数据集，使每次数据的顺序都不同\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X,Y,mini_batch_size,seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            #选择一个minibatch\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "\n",
    "            #前向传播\n",
    "            A3 , cache = opt_utils.forward_propagation(minibatch_X,parameters)\n",
    "\n",
    "            #计算误差\n",
    "            cost = opt_utils.compute_cost(A3 , minibatch_Y)\n",
    "\n",
    "            #反向传播\n",
    "            grads = opt_utils.backward_propagation(minibatch_X,minibatch_Y,cache)\n",
    "\n",
    "            #更新参数\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters,grads,learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentun(parameters,grads,v,beta,learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 \n",
    "                parameters , v , s = update_parameters_with_adam(parameters,grads,v,s,t,learning_rate,beta1,beta2,epsilon)\n",
    "        #记录误差值\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            #是否打印误差值\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(\"第\" + str(i) + \"次遍历整个数据集，当前误差值：\" + str(cost))\n",
    "    #是否绘制曲线图\n",
    "    if is_plot:\n",
    "        plt.plot(costs)\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('epochs (per 100)')\n",
    "        plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次遍历整个数据集，当前误差值：0.6906452511852934\n",
      "第1000次遍历整个数据集，当前误差值：0.3617644580722811\n",
      "第2000次遍历整个数据集，当前误差值：0.3551988505178231\n",
      "第3000次遍历整个数据集，当前误差值：0.19985720285975273\n",
      "第4000次遍历整个数据集，当前误差值：0.14491313295819366\n",
      "第5000次遍历整个数据集，当前误差值：0.11604790231218427\n",
      "第6000次遍历整个数据集，当前误差值：0.13687137403463995\n",
      "第7000次遍历整个数据集，当前误差值：0.03604251127691838\n",
      "第8000次遍历整个数据集，当前误差值：0.13256052788758488\n",
      "第9000次遍历整个数据集，当前误差值：0.17587054937060373\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEWCAYAAAAXa4wFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7Pk/wFjd2ckDsSaOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#使用普通的梯度下降\n",
    "layers_dims = [train_X.shape[0],5,2,1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer=\"gd\",is_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 1 elements, which is not acceptable for use with 'x' with size 300, 'y' with size 300.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4231\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Then is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4232\u001b[0;31m                 \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4233\u001b[0m                 \u001b[0mn_elem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: RGBA sequence should have length 3 or 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5b4536e0160b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mopt_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AndrewNg 编程作业/第二课/第二周/opt_utils.py\u001b[0m in \u001b[0;36mplot_decision_boundary\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpectral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, data, **kwargs)\u001b[0m\n\u001b[1;32m   2862\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m         verts=verts, edgecolors=edgecolors, **({\"data\": data} if data\n\u001b[0;32m-> 2864\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2865\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4243\u001b[0m                         \u001b[0;34m\"acceptable for use with 'x' with size {xs}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                         \u001b[0;34m\"'y' with size {ys}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4245\u001b[0;31m                         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_elem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4246\u001b[0m                     )\n\u001b[1;32m   4247\u001b[0m                 \u001b[0;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'c' argument has 1 elements, which is not acceptable for use with 'x' with size 300, 'y' with size 300."
     ]
    }
   ],
   "source": [
    "#预测\n",
    "preditions = opt_utils.predict(train_X,train_Y,parameters)\n",
    "\n",
    "#绘制分类图\n",
    "plt.title(\"Model with Gradient Descent optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "opt_utils.plot_decision_boundary(lambda x: opt_utils.predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
